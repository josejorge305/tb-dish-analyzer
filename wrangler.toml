# ================== BASE (dev/default) ==================
name = "tb-dish-processor"
main = "index.js"
account_id = "68cbbc868956bf7166613432aee13dec"
compatibility_date = "2023-12-01"
compatibility_flags = ["nodejs_compat"]

[ai]
binding = "AI"

# Browser Rendering for in-house menu scraping
[browser]
binding = "BROWSER"

[durable_objects]
bindings = []

[[services]]
binding = "metrics_core"
service = "tb-metrics-core"

[vars]
ENV = "development"
WORKER_NAME = "tb-dish-processor"
GIT_SHA = "manual"
BUILT_AT = "2025-11-11T00:00:00Z"

# === Tuning / feature flags ===
FALLBACK_TRIGGER_UNDER = "50"
HITS_LIMIT = "25"

# === Providers ===
PROVIDERS = "edamam,spoonacular,openai"
PROVIDERS_PARSE = "zestful,openai"

# === External hosts ===
# Note: Menu scraping now uses in-house Cloudflare Browser Rendering only
# Apify and RapidAPI scrapers have been removed
ZESTFUL_RAPID_HOST = "zestful.p.rapidapi.com"
ZESTFUL_DAILY_CAP = "5500"
USDA_FDC_HOST = "api.nal.usda.gov"

# === Internal services / URLs ===
FATSECRET_PROXY_URL = "https://tummy-lexicon-api.onrender.com"
# PROXY_API_KEY moved to secrets - set via: wrangler secret put PROXY_API_KEY
OPENAI_MODEL_ALLERGEN = "gpt-4o-mini"
OPENAI_MODEL_NUTRITION = "gpt-4o-mini"
PARALLEL_LLM = "true"
FATSECRET_SCOPES = "image-recognition"


# --- D1 (default env) ---
[[d1_databases]]
binding = "D1_DB"
database_name = "tb-database"
database_id = "055f6987-4353-4752-a4c5-24525e2f5cbb"

[[kv_namespaces]]
binding = "MENUS_CACHE"
id = "f363caa0518e46daa062f511c9d50c75"

[[kv_namespaces]]
binding = "USER_PREFS_KV"
id = "cff5be596e5048059c4b60fe78c49578"
preview_id = "000694bd5dfb422180cb026d4de957dd"

[[kv_namespaces]]
binding = "MENU_CLASSIFIER_CACHE"
id = "923f217b4b474a6b82b4b1063da97ea5"

[[kv_namespaces]]
binding = "DISH_ANALYSIS_CACHE"
id = "e7dc29dabe404495af66b04897527607"

# --- R2 (default env) ---
[[r2_buckets]]
binding = "R2_BUCKET"
bucket_name = "tb-analysis-cache"

# --- QUEUE PRODUCER (default env) ---
# Note: Queue consumer is only in production to avoid conflicts
# (a queue can only have one consumer at a time)
[[queues.producers]]
binding = "ANALYSIS_QUEUE"
queue = "tb-dish-analysis-queue"



# ================== PRODUCTION ENV ==================
[env.production]
# existing production bindings stay above this if you have them (KV, D1, etc.)
[env.production.ai]
binding = "AI"

# Browser Rendering for in-house menu scraping (production)
[env.production.browser]
binding = "BROWSER"

[env.production.durable_objects]
bindings = []

[[env.production.services]]
binding = "metrics_core"
service = "tb-metrics-core"

[env.production.vars]
ENV = "production"
WORKER_NAME = "tb-dish-processor-production"
GIT_SHA = "manual"
BUILT_AT = "2025-11-11T00:00:00Z"
# === Tuning / feature flags ===
FALLBACK_TRIGGER_UNDER = "50"
HITS_LIMIT = "25"

# === Providers ===
# Code reads ONLY this var for recipe provider order:
# (edamam, spoonacular, openai) â€” adjust as needed
PROVIDERS = "edamam,spoonacular,openai"
PROVIDERS_PARSE = "zestful,openai"

# === External hosts ===
# Note: Menu scraping now uses in-house Cloudflare Browser Rendering only
# Apify and RapidAPI scrapers have been removed
ZESTFUL_RAPID_HOST = "zestful.p.rapidapi.com"
ZESTFUL_DAILY_CAP  = "5500"
USDA_FDC_HOST      = "api.nal.usda.gov"

# === Internal services / URLs ===
FATSECRET_PROXY_URL = "https://tummy-lexicon-api.onrender.com"
# PROXY_API_KEY moved to secrets - set via: wrangler secret put PROXY_API_KEY
OPENAI_MODEL_ALLERGEN = "gpt-4o-mini"
OPENAI_MODEL_NUTRITION = "gpt-4o-mini"
PARALLEL_LLM = "true"
FATSECRET_SCOPES = "image-recognition"


# === Secrets to set (do NOT put values here; set via `wrangler secret put ...`) ===
#   wrangler secret put ADMIN_SECRET         # Admin key for cache/R2 deletion endpoints (X-Admin-Key header)
#   wrangler secret put PROXY_API_KEY        # FatSecret proxy API key
#   wrangler secret put OPENAI_API_KEY
#   wrangler secret put SPOONACULAR_KEY
#   wrangler secret put ZESTFUL_RAPID_KEY
#   wrangler secret put EDAMAM_APP_ID
#   wrangler secret put EDAMAM_APP_KEY
#   wrangler secret put GOOGLE_PLACES_API_KEY  # Google Places API for restaurant search
# Note: APIFY_TOKEN and RAPIDAPI_KEY are no longer needed - using in-house scraper

# --- D1 (production) ---
[[env.production.d1_databases]]
binding = "D1_DB"
database_name = "tb-database"
database_id = "055f6987-4353-4752-a4c5-24525e2f5cbb"

[[env.production.kv_namespaces]]
binding = "MENUS_CACHE"
id = "f363caa0518e46daa062f511c9d50c75"

[[env.production.kv_namespaces]]
binding = "USER_PREFS_KV"
id = "cff5be596e5048059c4b60fe78c49578"

[[env.production.kv_namespaces]]
binding = "MENU_CLASSIFIER_CACHE"
id = "923f217b4b474a6b82b4b1063da97ea5"

[[env.production.kv_namespaces]]
binding = "DISH_ANALYSIS_CACHE"
id = "e7dc29dabe404495af66b04897527607"

# --- R2 (production) ---
[[env.production.r2_buckets]]
binding = "R2_BUCKET"
bucket_name = "tb-analysis-cache"

# --- QUEUE PRODUCER (production) ---
[[env.production.queues.producers]]
binding = "ANALYSIS_QUEUE"
queue = "tb-dish-analysis-queue"

# --- QUEUE CONSUMER (production only) ---
[[env.production.queues.consumers]]
queue = "tb-dish-analysis-queue"
max_batch_size = 10
max_batch_timeout = 30
max_retries = 3

# --- CRON TRIGGERS (production) ---
# Every 5 minutes: process seeding ticks and daypart sampling jobs
# Daily at 3 AM UTC: menu cache refresh and daypart promotions
[env.production.triggers]
crons = ["*/5 * * * *", "0 3 * * *"]

# --- SERVICE BINDING (production) ---
# If you self-call this Worker (fetch to SELF), this binding matches prod name above.
